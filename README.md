# Data Engineering Zoomcamp
This project includes multiple projects done during a data engineering zoomcamp.

### [Project 1: Project Setup]
Setting up the tools and infrastructure needed for the project
* Setup GCP account
* Install Docker and docker-compose 
* Running Postgres locally with Docker
* Install Terraform
* Setting up infrastructure(datalake(GCS) and bigquery) on GCP with Terraform




### [Project 2: Data ingestion]
Ingesting data into the datalake and bigquery using airflow
* Setting up Airflow with Docker-Compose
* Extraction: Download and unpack the data
* Pre-processing: Convert this raw data to parquet
* Upload the parquet files to GCS
* Create an external table in BigQuery




### [Project 3: Data Warehouse]





### [Week 4: Analytics engineering]




### [Week 5: Batch processing](week_5_batch_processing)



### [Week 6: Streaming](week_6_stream_processing)




### Architecture diagram
<img src="images/architecture/arch_1.jpg"/>

### Technologies
* *Google Cloud Platform (GCP)*: Cloud-based auto-scaling platform by Google
  * *Google Cloud Storage (GCS)*: Data Lake
  * *BigQuery*: Data Warehouse
* *Terraform*: Infrastructure-as-Code (IaC)
* *Docker*: Containerization
* *SQL*: Data Analysis & Exploration
* *Airflow*: Pipeline Orchestration
* *dbt*: Data Transformation
* *Spark*: Distributed Processing
* *Kafka*: Streaming



## Tools 

Below are the following tools used during this project:

* Docker and Docker-Compose
* Python 3 (e.g. via [Anaconda](https://www.anaconda.com/products/individual))
* Google Cloud SDK 
* Terraform


